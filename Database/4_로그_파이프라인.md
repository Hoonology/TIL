# 로그 파이프라인
# INDEX

---

<br>

PostgreSQL 데이터베이스를 생성하고, SQL문을 이용해 데이터를 넣고, 넣은 결과를 확인합니다.
1. Postgres SQL 설치  

아래 명령어를 이용해서 PostgreSQL을 설치한다.  
``` bash
brew install postgresql
```

 

설치가 완료 되었으면 아래 명령어를 이용해 postgresql 서비스를 시작한다.

``` bash
brew services start postgresql
```
이렇게 하면 PostgreSQL 서버가 백그라운드에서 시스템 서비스로 시작됩니다.

다음 명령을 사용하여 서버를 수동으로 시작할 수도 있습니다.

```bash
pg_ctl -D /opt/homebrew/var/postgres start
```
서버 중지하려면 다음 실행
```bash
brew services stop postgresql
```
또는 수동으로 
``` bash
pg_ctl -D /opt/homebrew/var/postgres stop
```

버전 확인
```bash
pg_config --version
```

스프린트의 순서는 다음과 같습니다.

1. nginx 웹 서버의 로그를 확인합니다.
2. 프로그래밍 언어를 이용해 로그로부터 유의미한 데이터를 추출하여, 각 필드를 구분합니다. (변환)
- 이 프로그램을 파서(parser)라고 부르겠습니다.
3. PostgreSQL 데이터베이스 내에 로그를 적재할 테이블을 만듭니다.
4. 프로그래밍 언어를 이용해 PostgreSQL에 접근하고, 데이터를 넣는 프로그램을 만듭니다. (적재)
- 이 프로그램을 수집기(collector)라고 부르겠습니다.
5. 로그를 추출하고, 파서를 이용해 변환하고, 수집기를 이용해 PostgreSQL에 적재합니다.

요구사항
- 웹 서버에 접속 이벤트를 발생시키고, 로깅이 되는지 확인합니다.
- 파서를 직접 작성합니다.
- 데이터 웨어하우스에 로그를 적재할 공간을 준비합니다.
- 수집기를 직접 작성하여, 데이터 웨어하우스에 로그를 쌓을 수 있도록 합니다.
- 파이프라인으로 연결하여 스트리밍되는 로그를 추출/변환/적재합니다.


---
<br>

# 스프린트 진행
## 1. nginx 로그 분석
레포 안에 ```sample.log``` 파일은 테스트 해볼 수 있는 간단한 nginx 로그이다.
- 어떤 정보가 담겨있나요 ?  

    ```10.0.210.17 - - [28/Nov/2022:11:33:28 +0900] "GET /hello HTTP/1.1" 200 615 "-" "curl/7.84.0" "-"```
- 자바스크립트 언어를 이용해 한 줄로부터 출발지 IP, HTTP 메소드, 응답 코드, 주소, 접속 시간을 추출해 낼 수 있나요?
    ```js
    const logLine = '10.0.210.17 - - [28/Nov/2022:11:33:28 +0900] "GET /hello HTTP/1.1" 200 615 "-" "curl/7.84.0" "-"';

    const logParts = logLine.split(' ');

    const sourceIP = logParts[0];
    const httpMethod = logParts[5].slice(1);
    const responseCode = logParts[8];
    const address = logParts[6];
    const accessTime = logParts[3].slice(1);

    console.log(`Source IP: ${sourceIP}`);
    console.log(`HTTP Method: ${httpMethod}`);
    console.log(`Response Code: ${responseCode}`);
    console.log(`Address: ${address}`);
    console.log(`Access Time: ${accessTime}`);
    ```
    ```yaml
    Source IP: 10.0.210.17
    HTTP Method: GET
    Response Code: 200
    Address: /hello
    Access Time: 28/Nov/2022:11:33:28
    ```

## 2. 파서(parser) 작성
```parser.js``` 를 살펴보면,이 프로그램은 표준 입력을 필요로하는 프로그램입니다. 
``` js
#!/usr/bin/env node

const dayjs = require('dayjs')
const customParseFormat = require('dayjs/plugin/customParseFormat')
dayjs.extend(customParseFormat)

process.stdin.on("data", data => {
  let raw = data.toString()

  let regex = /\[(.+)\]/g
  let match = regex.exec(raw)[1]
  let timestamp = dayjs(match, 'DD/MMM/YYYY:hh:mm:ss +ZZ').toISOString()

  let source_ip = '127.0.0.1'
  let method = 'POST'
  let status_code = 404
  let path = '/replace-me'

  let jsonString = `
{
  "source_ip": "${source_ip}",
  "method": "${method}",
  "status_code": ${status_code},
  "path": "${path}",
  "timestamp": "${timestamp}"
}`

  process.stdout.write(jsonString)
})
```

실행 권한을 주고 다음과 같이 실행합니다.  
- 물론 실행 과정에서 npm 모듈 설치가 필요합니다.
```bash
cd ~/sprint-data-pipeline
npm install
```
``` bash
cat sample.log | ./parser.js
```
> 이 파이프라인은 cat 명령을 실행하여 sample.log라는 파일의 내용을 출력한 다음 |(파이프) 연산자를 사용하여 parser.js 스크립트로 파이프하는 명령입니다.  
 이 파이프라인의 목적은 파싱 및 분석을 위해 sample.log 파일의 내용을 parser.js 스크립트에 공급하는 것입니다.  

```bash
{
  "source_ip": "127.0.0.1",
  "method": "POST",
  "status_code": 404,
  "path": "/replace-me",
  "timestamp": "2022-11-28T02:33:28.000Z"
}
```

이 프로그램은 표준 입력의 첫번째 줄을 해석하고, JSON의 형태로 출력하는 기능을 가지고 있습니다. 그런데, parser.js를 살펴보면 알 수 있겠지만, 위의 출력은 사실 하드코딩된 결과물입니다. (위에서 유일하게 제대로 나오는 값은 날짜timestamp 하나 뿐입니다)

<br>

1. ```parser.js``` 확인하여 ```sample.log```의 내용에 맞게 수정해준다.
- ```parser.js```의 값들을 바꾼다.
```js
  let source_ip = '127.0.0.1'
  let method = 'POST'
  let status_code = 404
  let path = '/replace-me'

  let jsonString = `
```
```js
  let source_ip = '10.0.210.17'
  let method = 'GET'
  let status_code = 200
  let path = '/hello'
  let jsonString = `
```
- 값의 변화를 확인한.
```bash
cat sample.log | ./parser.js

{
  "source_ip": "10.0.210.17",
  "method": "GET",
  "status_code": 200,
  "path": "/hello",
  "timestamp": "2022-11-28T02:33:28.000Z"
}
```

## 3. 데이터 웨어하우스 준비
[ElephantSQL](https://www.elephantsql.com/)
인스턴스 생성완료
![pic](./assets/%EC%9D%B8%EC%8A%A4%ED%84%B4%EC%8A%A4%EC%83%9D%EC%84%B1%EC%99%84%EB%A3%8C.png)


## 4. 데이터베이스 연결 테스트
``` bash
├── sql                        // 테스트해볼 만한 SQL문 모음입니다.
│   ├── 1_reset.sql
│   ├── 2_describe_table.sql
│   └── 3_display_table_data.sql
└── sql-runner.js              // SQL문을 실행시켜서 결과를 콘솔에 출력합니다.
```

```sql-runner```는 sql 파일에 기록된 쿼리를``` PostgreSQL```에 전달해서 실행하는 역할을 합니다.  
```sql-runner.js``` 에 <U>실행 권한</u>을 주면 실행이 가능합니다.

그러나, 우리는 아직 연결을 위해 필요한 정보들을 프로그램에 알려주지 않았습니다. 따라서, .env 파일을 하나 생성해 환경 설정을 진행해야 합니다. 앞서 ElephantSQL의 인스턴스 정보의 URL 부분에서, 접속 정보를 확인하고, 다음과 같이 입력합시다. (여러분의 연결 정보에 따라 다르게 입력해야 합니다)

내 URL :  

postgres://ndlpqwue:lhPuGvLgytn6ipHonMhoIWd51Jkv4h9z@topsy.db.elephantsql.com/ndlpqwue  

```.env.example``` 파일을 열어서 수정한다. 이 때 파일 이름도 ```.env```로 바꿔야한다.
```bash
HOSTNAME=topsy.db.elephantsql.com 
USERNAME2=ndlpqwue
PASSWORD=lhPuGvLgytn6ipHonMhoIWd51Jkv4h9z
DATABASE=ndlpqwue
```
아래와 같이 파일을 열어 내용을 살펴본다.
```bash
./sql-runner.js < sql/1_reset.sql
./sql-runner.js < sql/2_describe_table.sql 
```
![pic](./assets/env.png)

<br>

## 5. 수집기(Collector) 작성 
앞서 살펴본 sql-runner로 데이터베이스에 접속해 테이블을 만들었다면, 드디어 로그를 적재할 준비가 끝난 것입니다. 여러 가지 sql 파일 중 3번 파일은, 테이블을 조회하는 SQL 쿼리문입니다. 지금은 아무것도 테이블에 기록하지 않았으므로, 결과는 다음과 같이 출력됩니다.

```bash
./sql-runner.js < sql/3_display_table_data.sql 

SELECT * FROM nginx;

┌─────────┐
│ (index) │
├─────────┤
└─────────┘

데이터베이스 연결 닫는 중...
데이터베이스 연결 종료
```
- 수집기 ```collector.js``` : ```sql-runner.js```와 거의 비슷한 구조를 가지고 있지만, 특별히 <u>웹 서버 로그를 nginx 테이블에 기록</u>하는 데에 초점이 맞춰져 있는 프로그램입니다.

- 프로세스가 멈추지 않고 실행되며, 표준 입력(stdin)을 스트림으로 받아, 들어오는 모든 로그를 즉시 데이터베이스로 보냅니다. 즉, 로그를 스트리밍합니다.
```bash
cat sample.json | ./collector.js

      INSERT INTO public.nginx (source_ip)
      VALUES (
        '{"source_ip":"127.0.0.1","method":"POST","status_code":404,"path":"/replace-me","timestamp":"2022-11-28T02:33:28.000Z"}'
      );
```

JSON 형식을 맞춰 수집기를 통해 적재된 데이터가 정상적인지 확인

```
./sql-runner.js < sql/3_display_table_data.sql
```
```./sql-runner.js < sql/3_display_table_data.sql``` 명령은 SQL 스크립트 ```3_display_table_data.sql``` 을 실행하고 SQL 쿼리 결과를 스크립트에 표시합니다

![정상](./assets/%EC%A0%95%EC%83%81.png)

JSON 문자열이 source_ip 필드에 그대로 들어가 있다. 각 필드의 목적에 맞게 JSON 으로 부터 값을 추출해서 입력해야한다. 이 수집기가 제대로 작동하게 수정해야한다.

>collector.js 코드는 현재 원시 JSON 문자열을 public.nginx 테이블의 source_ip 필드에 직접 삽입하고 있습니다. JSON의 값을 각 필드에 올바르게 삽입하려면 코드를 다음과 같이 수정해야 합니다.

```json
    let queryString = `
      INSERT INTO public.nginx (source_ip)
      VALUES (
        '${raw}'
      );
    `
```
- 먼저 JSON.parse()를 사용하여 원시 JSON 문자열을 JavaScript 객체로 구문 분석합니다. 그런 다음 개체 속성을 사용하여 각 필드의 적절한 값으로 INSERT 쿼리를 구성합니다.

```json
    let queryString = `
    INSERT INTO public.nginx (source_ip, method, status_code, path, timestamp)
    VALUES (
      '${json.source_ip}',
      '${json.method}',
      ${json.status_code},
      '${json.path}',
      '${json.timestamp}'
    );
  `
```
- 또한 누락된 필드(method, status_code, path 및 timestamp)를 INSERT 문에 추가했으며 쿼리 문자열에서 해당 값의 형식을 적절하게 지정했습니다(예: 문자열 값에 따옴표 사용). 숫자 값에 대한 따옴표 없음).  
- INSERT 쿼리를 구성할 때 raw 변수 대신 19행에서 선언된 json 변수를 사용하려면 다음과 같이 21행에서 queryString 변수를 업데이트할 수 있습니다.


- 마지막으로 SIGINT 이벤트 핸들러에서 client.end() 호출을 수정하여 await client.end() 대신 await client.end()를 사용했습니다. end()`는 원래 코드에 오타가 있었기 때문입니다.

- timestamp 필드에 대해 Postgres에서 to_timestamp 함수를 사용하려면 다음과 같이 21행에서 queryString 변수를 업데이트할 수 있습니다.
  ```json
    let queryString = `
    INSERT INTO public.nginx (source_ip, method, status_code, path, timestamp)
    VALUES (
      '${json.source_ip}',
      '${json.method}',
      ${json.status_code},
      '${json.path}',
      to_timestamp('${json.timestamp}', 'YYYY-MM-DD"T"HH24:MI:SS.ff3"Z"')
   );
  `;

  ```

  여기서는 json.timestamp 값을 첫 번째 인수로 사용하고 'YYYY-MM-DD"T"HH24:MI:SS.ff3"Z"' 형식 문자열과 함께 to_timestamp 함수를 사용하고 있습니다. 예상 타임스탬프 형식을 지정하는 두 번째 인수로.


  형식 문자열은 T 및 Z 문자를 포함하여 json.timestamp 값의 형식과 일치해야 합니다. 또한 ff3 형식 코드는 타임스탬프 값의 밀리초 부분을 구문 분석하는 데 사용됩니다.

## 6. 잘못 쌓인 데이터 지우기 (optional)
선택적으로 도전할 수 있는 부분입니다.

5번 과정에서 수집기를 테스트하느라 데이터베이스에 필드에 맞지 않는 행(row)들이 잔뜩 쌓여 있을 것으로 예상됩니다. 편하게, sql/1_reset.sql 파일을 이용해서 아예 새롭게 테이블을 날려버리고 다시 만들 수도 있지만, 테이블의 콘텐츠만 지워버리는 명령을 추가해서 실행해보면 더 좋겠습니다.  
```./sql-runner.js < sql/1_reset.sql```

4_clean_up_table.sql 에 SQL 구문을 입력하고, sql-runner를 이용해 테이블을 깨끗하게 지워보세요.

## 7. 파이프라인 완성

수집기가 정상적으로 작동한다면, 테이블 조회 시 결과가 다음과 같이 필드에 맞게 기록될 것입니다.
![dbstatus](./assets/dbstatus.png)
![logic](./assets/logic.png)

이제 nginx를 실행시키고, access.log에 로그가 쌓이는 것을 확인한 후, 파서와 수집기를 연결해 로그 파이프라인을 완성시켜 봅시다.  

- 먼저 nginx가 실행중이어야 합니다.
  - ```bash
    brew services restart nginx
    ╰─$ brew services
    sudo find / -name "access.log" 안나옴
    /opt/homebrew/var/log/nginx 여깃넹

    ```
- access.log 파일을 관찰합니다. tail -f 명령어를 사용합니다.
  - ```bash
      127.0.0.1 - - [31/Mar/2023:10:28:05 +0900] "GET /article HTTP/1.1" 404 555 "http://localhost:10024/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36"
      127.0.0.1 - - [31/Mar/2023:10:28:15 +0900] "GET /article HTTP/1.1" 404 555 "http://localhost:10024/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36"
      127.0.0.1 - - [31/Mar/2023:10:28:20 +0900] "GET /article HTTP/1.1" 404 555 "http://localhost:10024/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36"
      127.0.0.1 - - [31/Mar/2023:10:38:00 +0900] "GET /article HTTP/1.1" 404 555 "http://localhost:10024/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36"
      127.0.0.1 - - [31/Mar/2023:10:38:01 +0900] "GET /article HTTP/1.1" 404 555 "http://localhost:10024/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36"
      127.0.0.1 - - [31/Mar/2023:10:38:11 +0900] "GET /article HTTP/1.1" 404 555 "http://localhost:10024/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36"
      127.0.0.1 - - [31/Mar/2023:10:38:31 +0900] "GET /article HTTP/1.1" 404 555 "http://localhost:10024/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36"
      127.0.0.1 - - [31/Mar/2023:10:38:52 +0900] "GET /article HTTP/1.1" 404 555 "http://localhost:10024/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36"
      127.0.0.1 - - [31/Mar/2023:10:39:03 +0900] "GET /article HTTP/1.1" 404 555 "http://localhost:10024/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36"
      127.0.0.1 - - [31/Mar/2023:10:39:12 +0900] "GET /article HTTP/1.1" 404 555 "http://localhost:10024/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36"
     ```
- 웹 서버에 다양한 접속 기록을 생성합니다. Postman이든 curl이든 웹 브라우저든 상관없습니다.
- 접속 시, ```access.log``` 파일에 새로운 로그가 출력되는 지 확인합니다.
- 4번에 성공했다면, ```tail -f access.log``` 의 표준 출력(stdout)이 파서의 표준 입력(stdin)이 되도록 파이프(|)를 이용해 연결합니다.
  - ```tail -f access.log | node parser.js```
  - 다음과 같은 화면을 기대할 수 있습니다.
  - ```tail -f -0 /<nginx_로그_경로>/access.log | ./parser.js | ./collector.js```
새로운 출력만 로그로 쌓을 수 있도록 tail -f 외에도 -0 옵션도 같이 주어야 합니다.

## 8. 제출
제출 시, stdout.log 라는 이름의 파일을 하나 만들어   
```./sql-runner.js < sql/3_display_table_data.sql``` 의 결과를 복사/붙여넣기 해서 첨부해주세요.

따라서 다음 세 개의 파일이 반드시 완성된 상태로 있어야 합니다.

파서 parser.js
수집기 collector.js
테이블 적재 현황 로그 stdout.log
다음은 선택 제출 사항입니다.

sql/4_clean_up_table.sql